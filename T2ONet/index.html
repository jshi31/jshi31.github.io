
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:16px;
		margin-left: auto;
		margin-right: auto;
		width: 800px;
	}
	
	h1 {
		font-weight:300;
	}
		
	h2 {
		font-weight:300;
		font-size: 22px;
		text-align: left;
	}

	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	pre {
    text-align: left;
    white-space: pre;
	background-color: ghostwhite;
	border: 1px solid #CCCCCC;
	padding: 10px 20px;
	margin: 10px;
    tab-size:         4; /* Chrome 21+, Safari 6.1+, Opera 15+ */
    -moz-tab-size:    4; /* Firefox 4+ */
    -o-tab-size:      4; /* Opera 11.5 & 12.1 only */
  	}

</style>

<html>
  <head>
		<title>Learning by Planning: Language-Guided Global Image Editing</title>
		<meta property="og:image" content=""/>
		<meta property="og:title" content="FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding" />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">Learning by Planning: Language-Guided Global Image Editing</span>
	  		  <table align=center width=600px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://www.cs.rochester.edu/u/jshi31/">Jing Shi</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://sites.google.com/view/ningxu/">Ning Xu</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://www.linkedin.com/in/yihang-xu-341790118">Yihang Xu</a></span>
		  		  		</center>
		  		  	  </td>
		  		  	</tr>
		  		  	<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://francky.me/research.php">Frank Dernoncourt</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a></span>
						</center>
					</td>
				</tr>
				</table>
          		<!-- <span style="font-size:30px">ECCV 2016.</span> -->

			  <table align=center width=600px>
				  <tr>
					  <td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://www.cs.rochester.edu/">University of Rochester</a></span>
						</center>
					  </td>
					  <td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="https://research.adobe.com/">Adobe Research</a></span>
						</center>
					  </td>
			  </table>
			IEEE Conference on Computer Vision and Pattern Recognition (<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR</a>) 2021
          </center>

   		  <br><br>
		  <hr>

  		  <br>
  		  <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<a href="./resources/images/teaser.jpg"><img class="rounded" src = "./resources/images/teaser.jpg" width="600px"></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					<center>
  	                	<span style="font-size:14px">Overview of our task and method. <i>Language-Guided Global Image Editing</i>: given the input image and the request, we predict a sequence of actions to edit the image progressively with a series of intermediate images generated. And the final edited image is our output, which should accord with the request. <i>Operation Planning</i>: the input image and target image are given, and we plan a sequence of action to make the final edited image reach the target image. Since we only have the supervision of the target image, we obtain the planed action sequences via operation planning as pseudo ground-truth, which is used to train our model text-to-operation network (T2ONet).
					</center>
  	              </td>

  		  </table>
      	  <br><br>

		  <table align=center width=720px>
			<!-- <center><h1>Download</h1></center> -->
			<tr>
				<td width=300px>
					<center>
						<a href="#download"><img class="rounded" onmouseover="this.src='./resources/images/data_icon.png';" onmouseout="this.src='./resources/images/data_icon.png';" src = "./resources/images/data_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">Download Dataset</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#video"><img class="rounded" onmouseover="this.src='./resources/images/video_icon.png';" onmouseout="this.src='./resources/images/video_icon.png';" src = "./resources/images/video_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">Video</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#analysis"><img class="rounded" onmouseover="this.src='./resources/images/magnify_glass.png';" onmouseout="this.src='./resources/images/magnify_glass.png';" src = "./resources/images/magnify_glass.png" height = "120px"></a><br>
						<span style="font-size:16px">Visualization</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
					  <a href="https://github.com/jshi31/T2ONet"><img class="rounded" onmouseover="this.src='./resources/images/github_icon.png';" onmouseout="this.src='./resources/images/github_icon.png';" src = "./resources/images/github_icon.png" height = "120px"></a><br>
					  <span style="font-size:16px">GitHub Repo</span><br>						
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>

		  <hr>

  		  <table align=center width=720px>
				<center><h1>Abstract</h1></center>
		  </table>
		  <center>
			<span>
		 	Recently, language-guided global image editing draws increasing attention with growing application potentials. However, previous GAN-based methods are not only confined to domain-specific, low-resolution data but also lacking in interpretability. To overcome the collective difficulties, we develop a text-to-operation model to map the vague editing language request into a series of editing operations, e.g., change contrast, brightness, and saturation. Each operation is interpretable and differentiable. Furthermore, the only supervision in the task is the target image, which is insufficient for a stable training of sequential decisions. Hence, we propose a novel operation planning algorithm to generate possible editing sequences from the target image as pseudo ground truth. Comparison experiments on the newly collected MA5k-Req dataset and GIER dataset show the advantages of our methods.
		  	</span>
		  </center>
  		  <br><br>
		  <hr>

<!-- 		  <table align=center width=720px>
			<center><h1>Demo video</h1></center>
			<tr>
				<table align=center width=720px>
					<tr>
						<td align=center width=720px>
							<iframe width="600" height="320" src="https://www.youtube.com/embed/notLDzBJ2mg" frameborder="0" allowfullscreen></iframe>
						</td>
					  </tr>
					<tr>
						<td align=center width=720px>
						  <span style="font-size:14px"><i>
							An illustrative video of FineGym's hiecharcial annotations given a complete competition.
							Action and subaction boundaries are highlighted while irrelevant fragments are fast-forwarded.
							We also present the tree-based process at the end of the demo video.</i>
						</span>
						   </td>
					  </tr>
					 </table>
			  </tr>
		  </table> -->
		   <!-- <br><br> -->
		  <!-- <hr> -->
		<table align=center width=720px>
			<tr>
				<td width=400px>
				  <left>
			<center><h1>Related Work</h1></center>
			<strong>We keep a curated paper list for language-guided image editing <a href=https://github.com/jshi31/awesome-language-guided-image-editing/>here.</href><a></strong>
			</left>
			</td>
		 </tr>

		</table>
		  <table id="video" align=center width=720px>
			<center><h1>5-Minute presentation video</h1></center>
			<tr>
				<table align=center width=720px>
					<tr>
						<td align=center width=720px>
							<iframe width="600" height="320" src="https://www.youtube.com/embed/5CZpK1WED3E" frameborder="0" allowfullscreen></iframe>
						</td>
					  </tr>
					<tr>
					 </table>
			  </tr>
		  </table>
		   <br><br>
		  <hr>


<!-- 		  <table align=center width=720px>
			<center><h1>Dataset hierarchy</h1></center>
			<tr>
				<td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/hierarchy.png" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
				<td width=400px>
				  <center>
					  <span style="font-size:14px"><i>
						FineGym organizes both the semantic and temporal annotations hierarchically.
						The upper part shows three levels of categorical labels, namely events (e.g. balance beam), sets (e.g. dismounts) and elements (e.g. salto forward tucked).
						The lower part depicts the two-level temporal annotations, i.e. the temporal boundaries of actions (in the top bar) and sub-action instances (in the bottom bar).
						</i>
				</center>
				</td>

		  </table>
	      <br><br>
		  <hr> -->

		  <table align=center width=720px>
			<center><h1>Intermediate actions and images visualization</h1></center>
			<tr>
				<center>
				<span>
					We present several examples of operation planning process and image editing process.
				</span>
				<br>
				</center>
				<td width=720px>
					<center>
						<span style="font-size:22px"><a>Operation Planning</a></span><br>
						<a><img onmouseover="this.src='./resources/examples/planning.gif';" onmouseout="this.src='./resources/examples/planning.gif';" src = "./resources/examples/planning.gif" width = "720px"></img></a>
					</center>
			</tr>
			<tr>
				<td width=720px>
					<center>
						<span style="font-size:22px"><a>Language-Guided Image Editing</a></span><br>
						<a><img onmouseover="this.src='./resources/examples/editing.gif';" onmouseout="this.src='./resources/examples/editing.gif';" src = "./resources/examples/editing.gif" width = "720px"></img></a>
					</center>
				</td>

			</tr>
		  </table>

		  <br><br>
		  <hr>

		  <table align=center width=720px>
			<center><h1>Visual comparison with other methods</h1></center>
			<tr>
				<center>
				<br>
				</center>
				<td width=720px>
					<center>
						<span><a>Visualization for comparison of our method T2ONet with other methods on MA5k-Req (left) and GIER (right)</a></span><br>
  	                	<a href="./resources/examples/comparison.png"><img class="rounded" src = "./resources/examples/comparison.png" width="720px"></img></href></a><br>
					</center>
			</tr>
		  </table>

		  <br><br>
		  <hr>

		  <table id='analysis' align=center width=720px>
			<center><h1>Methodological Advantages and Extensions</h1></center>
			<table>
			<center><h2> (1) Resolution Independent. </h2></center>
			<tr>
				  <td width=400px>
				  <center>

					  <a><img class="rounded" src = "./resources/examples/resolution.png" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align=center width=720px>
				  	<span style="font-size:14px"><i>
					Compared with the GAN-based method GeNeVa and
					Pix2pixAug, although all the methods conduct the correct editing,
					our method has no pixel distortion and is independent to image
					resolution. This is because our editing operation is resolution-independent.</i>
					</span>
				</td>
			</tr>
			</table>

			<br>

			<table>
			<center><h2> (2) Generate multiple pissible outputs. </h2></center>
			<tr>
				  <td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/examples/multi-output.png" width="800px"></img></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align=center width=720px>
				  <span style="font-size:14px"><i>
					Visualization for diversified output given the same input and request by sampling the operation parameter at inference
					stage.</i>
				</span>
				</td>
			</tr>
			</table>

			<br>

			<table>
				<center><h2> (3) Extension: Planning for local editing. </h2></center>
				<tr>
					  <td width=400px>
					  <center>
						  <a><img class="rounded" src = "./resources/examples/plan_local.png" width="600px"></img></a><br>
					</center>
					</td>
				</tr>
				<tr>
					<td align=center width=720px>
					  <span style="font-size:14px"><i>
						By adding a segmentation model to get object masks and add the operation “inpainting”, the operation planning algorithm can be extended to local editing. The recovered output is the planning result that is similar to target image
					</i>
					</span>
					</td>
				</tr>
				</table>

			<br>

					
	      <br><br>
		  <hr>


		  <table id="download" align=center width=720px>
			<center><h1>Download</h1></center>
			<tr>
				<td width=300px>
					<center>
						<span style="font-size:24px">FiveK-Req Image & Annotation</span><br>
						<br>
						<a href="https://drive.google.com/drive/folders/14-NuoBfWPY2VFy7OOklBFqAmINLFMnUk?usp=sharing">
						<img class="rounded" onmouseover="this.src='./resources/images/dataset_icon.jpg';" onmouseout="this.src='./resources/images/dataset_icon.jpg';" src = "./resources/images/dataset_icon.jpg" height = "150px">
						</a><br>
						Check its README to see the data structure.
					<span style="font-size:16px"></span>
					</center>
				</td>
			</tr>
		</table>

		 <table align=center width=720px>
			<center><h1>Paper & Appendix</h1></center>
			   <tr>
				 <td align=center><a href=""><img class="layered-paper-big" style="height:160px" src="./resources/images/paper_pdf_thumb.jpg"/></a></td>
				 <td><span style="font-size:14pt">Jing Shi, Ning Xu, Yihang Xu, Trung Bui, Franck Dernoncourt, Chenliang Xu<br>
				 Learning by Planning: Language-Guided Global Image Editing<br>
				 In CVPR, 2021.<br>
				 (<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Learning_by_Planning_Language-Guided_Global_Image_Editing_CVPR_2021_paper.pdf">Paper</a>)</a>
				 <span style="font-size:4pt"><a href=""><br></a>
				 </span>
			 </tr>
		   </table>
		 
		 <br><br>
		 <hr>

		  <table align=center width=720px>
			<center><h1>Cite</h1></center>
		  <div class="disclaimerbox">
			<!-- <center><h2>How to interpret the results</h2></center> -->

		   <span>
				<!-- <center><span style="font-size:28px"><b>Cite</b></span></center> -->
				<pre style = "font-family:Courier; font-size:11px">
@inproceedings{shi2021learning,
  title={Learning by Planning: Language-Guided Global Image Editing},
  author={Shi, Jing and Xu, Ning and Xu, Yihang and Bui, Trung and Dernoncourt, Franck and Xu, Chenliang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13590--13599},
  year={2021}
}
				</pre>
		  </div>
  		  </table>

			<br><br>
			<hr>
  
		  	
  		  <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
				This work was supported in part by an Adobe research gift, and NSF 1813709, 1741472 and 1909912. The article solely reflects the opinions and conclusions of its authors but not the funding agents.
				The template of this webpage is borrowed from <a href=https://richzhang.github.io/colorization/>Richard Zhang</href><a>.
			</left>
		</td>
			 </tr>
		</table>

		<br><br>
		<hr>


		<table align=center width=720px>
			<tr>
				<td width=400px>
				  <left>
			<center><h1>Contact</h1></center>
			For further questions and suggestions, please contact Jing Shi (<a href='mailto:j.shi@rochester.edu'>j.shi@rochester.edu</a>).
			
			
		</left>
	</td>
		 </tr>
	</table>

		<br><br>

<script>
	
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 
