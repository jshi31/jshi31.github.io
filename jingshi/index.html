<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Jing Shi</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<style type="text/css">


	    /* Color scheme stolen from Sergey Karayev */
	    
	    a {
	      color: #1772d0;
	      text-decoration: none;
	    }
	    
	    a:focus,
	    a:hover {
	      color: #f09228;
	      text-decoration: none;
	    }
	    
	    body,
	    td,
	    th,
	    tr,
	    p,
	    a {
	      font-family: 'Lato', Verdana, Helvetica, sans-serif;
	      font-size: 18px
	    }

	    
	    heading {
	      font-family: 'Lato', Verdana, Helvetica, sans-serif;
	      font-size: 26px;
	    }
	    
	    papertitle {
	      font-family: 'Lato', Verdana, Helvetica, sans-serif;
	      font-size: 18px;
	      font-weight: 700
	    }
	    
	    name {
	      font-family: 'Lato', Verdana, Helvetica, sans-serif;
	      font-size: 26px;
	    }
	    
	    .one {
	      width: 220px;
	      height: 220px;
	      position: relative;
	    }
	    
	    .two {
	      width: 220px;
	      height: 220px;
	      position: absolute;
	      transition: opacity .2s ease-in-out;
	      -moz-transition: opacity .2s ease-in-out;
	      -webkit-transition: opacity .2s ease-in-out;
	    }

	    .fade {
	      transition: opacity .2s ease-in-out;
	      -moz-transition: opacity .2s ease-in-out;
	      -webkit-transition: opacity .2s ease-in-out;
	    }
	    
	    span.highlight {
	      background-color: #ffffd0;
	    }
	    </style>
		<link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131122111-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-131122111-1');
		</script>

	</head>
	<body>
    <IMG ALIGN=TOP SRC="/images/urcslogo.gif">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/me_hawaii.jpg" width="200" alt="" /></span>
					  <h1>Jing Shi   施靖</h1>
						<p>Strike while the iron is hot<br />
						</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Introduction</a></li>
							<li><a href="#news">News</a></li>
							<li><a href="#publication">Publication</a></li>
							<li><a href="#experience">Experience</a></li>
							<li><a href="#course">Course</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Introduction</h2>
										</header>
										<p>I am a fifth-year CS Ph.D student at the University of Rochester, supervised by <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>. <br>
										  I got  my bechalor degree from the University of Electronic Sience and Techonology of China.<br>
									      My interest spans deep learning, machine learning, and computer vision.
									    </p>
								              <p align=center>
								                <a href="document/Jing_Shi_CV.pdf" class="icon fa-CV alt"> CV </a><span class="label">&nbsp&nbsp
								                <a href="https://scholar.google.com/citations?user=GPDTa9sAAAAJ&hl=en" class="icon fa-google alt"><span class="label">Google Scholar</span></a> &nbsp&nbsp
								                <a href="https://www.linkedin.com/in/jingshi31/" class="icon fa-linkedin alt"><span class="label">Linkedin</span></a>&nbsp&nbsp
								                <a href="https://github.com/jshi31" class="icon fa-github alt"><span class="label">Github</span></a>
								              </p>
									  </p>
										  <h2>Connection</h2>
										  <dl class="alt">
										    <dt>Office</dt>
										    <dd>Wegmans 2403</dd>
										    <dt>Email</dt>
										    <dd>j.shi(at)rochester(dot)edu</dd>
									</div>
								</div>
							</section>

						<!-- News -->
							<section id="news" class="main special">
								<header class="major">
									<h2>News</h2>
								</header> <p align="left">
								[Jul/2021] Three papers accepted by ICCV2021: two about language and scene understanding, one about language driven image editing.<br>
								[Mar/2021] Our language driven image editing paper is accepted by CVPR2021. <br>
								[Sep/2020] Our language driven image editing paper is accepted by ACCV2020 Oral. <br>
								[Feb/2019] Our weakly supervised video grounding paper is accepted by CVPR 2019. <br>
								</p>
							</section>

						<!-- Publication -->
							<section id="publication" class="main special">
								<header class="major">
									<h2>Publications</h2>
								</header>
								<ul class="features">
									<li></li>
								</ul>
								<table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
   								<tr>
						        <td>
						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

						        	<!--SpaceEdit CVPR22-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='SpaceEdit'><img src='images/spaceedit.jpg' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://arxiv.org/abs/2112.00180">
						            <papertitle>SpaceEdit: Learning a Unified Editing Space for Open-Domain Image Editing</papertitle>
						            </a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
						            <a href="https://www.cs.rochester.edu/u/hzheng15/haitian_homepage/index.html">Haitian Zheng</a>,
						            Alex Smith,
						            <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2022
						            <br>
						            <p></p>
						            <p align="left">We introduce a pretrained pix2pix model with a unified editing space that can work for various editing-related downstream tasks.</p>
						            </td>
						            </tr>

						        	<!--WS-SGG ICCV2021-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='CSSC'><img src='images/WS-SGG.png' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_A_Simple_Baseline_for_Weakly-Supervised_Scene_Graph_Generation_ICCV_2021_paper.pdf">
						            <papertitle>A Simple Baseline for Weakly-Supervised Scene Graph Generation</papertitle>
						        	</a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="http://pages.cs.wisc.edu/~yiwuzhong/">Yiwu Zhong</a>,
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
						            <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>International Conference on Computer Vision (ICCV)</em>, 2021
						            <br>
						            <br>					            
						            <p></p>
						            <p align="left">We construct a versatile baseline for Weakly-Supervised Scene Graph Generation.</p>
						            </td>
						            </tr>

						        	<!--Language to scene graph ICCV2021-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='CSSC'><img src='images/txt2sg.png' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://arxiv.org/abs/2109.02227">
						            <papertitle>Learning to Generate Scene Graph from Natural Language Supervision</papertitle>
						        	</a>
						            <br>
						            <a href="http://pages.cs.wisc.edu/~yiwuzhong/">Yiwu Zhong</a>,
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://jwyang.github.io/">Jianwei Yang</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>,
						            <br>
						            <em>International Conference on Computer Vision (ICCV)</em>, 2021
						            <br>
						            <a href="https://github.com/YiwuZhong/SGG_from_NLS">code</a> 
						            <br>					            
						            <p></p>
						            <p align="left">We generate scene graph from only natural language supervision.</p>
						            </td>
						            </tr>

						        	<!--GAN ICCV2021-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='CSSC'><img src='images/LGGIE.png' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Language-Guided_Global_Image_Editing_via_Cross-Modal_Cyclic_Mechanism_ICCV_2021_paper.pdf">
						            <papertitle>Language-Guided Global Image Editing via Cross-Modal Cyclic Mechanism</papertitle>
						        	</a> 
						            <br>
						            <a href="https://wtjiang98.github.io/">Wentao Jiang</a>,
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
									Jiayun Wang,
									Chen Gao,
									<papertitle>Jing Shi</papertitle>,
									<a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
									<a href="http://colalab.org/people">Si Liu </a>,
						            <br>
						            <em>International Conference on Computer Vision (ICCV)</em>, 2021
						            <br>

						            <br>					            
						            <p></p>
						            <p align="left">We tackle language-guided global image editing via GAN model with cross-modal consistency </p>
						            </td>
						            </tr>

						        	<!--Learn by Planning CVPR2021-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='CSSC'><img src='images/LearnByPlan.png' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Learning_by_Planning_Language-Guided_Global_Image_Editing_CVPR_2021_paper.pdf'>
						            <papertitle>Learning by Planning: Language-Guided Global Image Editing</papertitle>
						        	</a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
						            <a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a>,
						            <a href="http://francky.me/research.php">Franck Dernoncourt</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2021
						            <br>
						            <a href="https://jshi31.github.io/T2ONet">project page</a> / 
						            <a href="https://github.com/jshi31/T2ONet">code&dataset</a> 
						            <br>					            
						            <p></p>
						            <p align="left">We contruct a new framework for language-guided global image editing.</p>
						            </td>
						            </tr>


						        	<!--CSSC ICLR2020-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='CSSC'><img src='images/CSSC.jpg' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://arxiv.org/abs/2010.01381">
						            <papertitle>Cubic Spline Smoothing Compensation for Irregularly Sampled Sequences</papertitle>
						            </a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://jing-bi.github.io/">Jing Bi</a>,
						            <a href="https://yingrliu.com/">Yingru Liu</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>Arxiv Preprint</em>
						            <br>
						            <p></p>
						            <p align="left">We introduce the cubic spline smoothing compensation upon ODE-RNN model for irregularly observed time series.</p>
						            </td>
						            </tr>

						        	<!--SDN ICLR2020-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='NeurlSDE'><img src='images/NeurlSDE.jpg' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://arxiv.org/abs/2006.06145">
						            <papertitle>Learning Continuous-Time Dynamics by Stochastic Differential Networks</papertitle>
						            </a>
						            <br>
						            <a href="https://yingrliu.com/">Yingru Liu</a>,
						            Yucheng Xing, 
						            Xuewen Yang,
						            Xin Wang,
									<papertitle>Jing Shi</papertitle>,
									Di Jin,
									Zhaoyue Chen
						            <br>
						            <em>Arxiv Preprint</em>
						            <br>
						            <p></p>

						            </td>
						            </tr>


						        	<!--LDIE ACCV2020-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='LDIE'><img src='images/LDIE.png' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://arxiv.org/abs/2010.02330">
						            <papertitle>A Benchmark and Baseline for Language-Driven Image Editing  </papertitle>
						        	</a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
						            <a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a>,
						            <a href="http://francky.me/research.php">Franck Dernoncourt</a>,
						            <a href="http://www.zheng-wen.com">Zheng Wen</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>Asian Conference on Computer Vision (ACCV)</em>, 2020 <papertitle>(Oral)</papertitle>
						            <br>
						            <a href="https://github.com/jshi31/LDIE_ACCV">code</a> /
						            <a href="https://sites.google.com/view/gierdataset">dataset</a> 
						            <p></p>
						            <p align="left">We introduce the GIER dataset and a pipeline for language-driven image editing.</p>
						            </td>
						            </tr>

						            <!--EMGAN IJCAI2020-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='LDIE'><img src='images/GAN_EM.jpg' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://www.ijcai.org/Proceedings/2019/0612.pdf">
						            <papertitle>GAN-EM: GAN Based EM Learning Framework </papertitle>
						            </a>
						            <br>
						            <a href="https://www.linkedin.com/in/wentian-zhao-b3576815b/">Wentian Zhao*</a>,
						            <a href="https://scholar.google.com/citations?user=0l0fSYIAAAAJ&hl=en">Shaojie Wang*</a>,
						            Zhihuai Xie,
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2019
						            <br>
						            <a href="https://github.com/zhaowt61/GAN-EM">code</a>
						            <p></p>
						            </td>
						            </tr>

						        	<!--VidGround CVPR2019-->
						        	<tr>
						            <td width="25%">
						            <div class="two" id='vidground'><img src='images/vidground.png' width="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Not_All_Frames_Are_Equal_Weakly-Supervised_Video_Grounding_With_Contextual_CVPR_2019_paper.pdf">
						            <papertitle><i>Not All Frames Are Equal</i>: Weakly-Supervised Video Groundingwith Contextual Similarity and Visual Clustering Losses </papertitle>
						            </a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="http://pages.cs.wisc.edu/~jiaxu/">Jia Xu</a>,
						            <a href="http://boqinggong.info/">Boqing Gong</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019
						            <br>
						            <a href="http://www.cs.rochester.edu/~cxu22/r/videoground/">project page</a>/
						            <a href="https://github.com/jshi31/NAFAE">code</a>
						            <p></p>
						            <p align="left">We introduce contextual smilarity loss and visual clustering loss for weakly supervised video grounding.</p>
						            </tr>

						        	<!--AVE ECCV2018-->
						            <tr>
						            <td width="25%">
						            <div class="two" id='AVE_image'><img src='images/AVE.png' width="220" height="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf">
						            <papertitle>Audio-Visual Event Localization in Unconstrained Videos </papertitle>
						            </a>
						            <br>
						            <a href="http://yapengtian.org/">Yapeng Tian</a>,
									<papertitle>Jing Shi</papertitle>,
						            <a href="http://www2.ece.rochester.edu/~bli23/">Bochen Li</a>,
						            <a href="http://www2.ece.rochester.edu/~zduan/">Zhiyao Duan</a>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>European Conference on Computer Vision (ECCV)</em>, 2018
						            <br>
						            <a href="https://sites.google.com/view/audiovisualresearch">project page</a>/
									<a href="https://github.com/YapengTian/AVE-ECCV18">code</a>
						            <p></p>
						            <p align="left">We introduce audio-visual event localization to understand the interaction of vision and audio.</p>
						            </td>
						            </tr>

						        	<!--Boundary Control-->
						            <tr>
						            <td width="25%">
						            <div class="two" id='bnd_image'><img src='images/boundarycontrol.jpg' width="220" height="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://ieeexplore.ieee.org/document/7962253">
						            <papertitle>Boundary Vibration Control of Variable Length Crane Systems in Two Dimensional Space with Output Constraints </papertitle>
						            </a>
						            <br>
						            <a href="https://www.researchgate.net/profile/Xiuyu_He">Xiuyu He</a>,
						            <a href="https://www.researchgate.net/profile/Wei_He29">Wei He</a>,						            
									<papertitle>Jing Shi</papertitle>,
						            <a href=https://www.researchgate.net/profile/Changyin_Sun3>Changyin Sun</a>
						            <br>
						            <em>IEEE/ASME Transactions on Mechatronics (TMech)</em>, 2017
						            <br>
						            <p></p>
						            <p align="left">We demonstrate the lyapunov stability of the boundary control for the moving crane system in 2-D space.</p>
						            </tr>

						        	<!--Eddy Current-->
						            <tr>
						            <td width="25%">
						            <div class="two" id='AVE_image'><img src='images/eddycurrent.png' width="220" height="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="https://ieeexplore.ieee.org/document/7992029">
						            <papertitle>Impact Damage Detection and Characterization using Eddy Current Pulsed Thermography </papertitle>
						            </a>
						            <br>
						            <a href="https://scholar.google.com/citations?user=BJR0ZGsAAAAJ&hl=en">Yizhe Wang</a>,
						            Han Ke,						            
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://scholar.google.com/citations?hl=en&user=mYlmFbAAAAAJ&view_op=list_works&sortby=pubdate">Bing Gao</a>,
						            <a href="https://scholar.google.com/citations?user=Mh94SyYAAAAJ&hl=en">Guiyun Tian</a>
						            <br>
						            <em>IEEE Far East NDT New Technology & Application Forum (FENDT)</em>, 2016
						            <br>
						            <p></p>
						            <p align="left">We use eddy current pulse thermography to detect impact damages of torque arm in aircraft brake system.</p>
						            </tr>

						        </table>
        						</td>
    							</tr>
  								</table>
							<footer class="major"></footer>
							</section>

                        <!-- Experience Section -->
							<section id="experience" class="main special">
								<header class="major">
								<h2>Experience</h2>
								</header>
								<ul class="features">
									<li></li>
								</ul>
								<table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
   								<tr>
						        <td>

						        <!-- Project -->
						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						          <tr>
						            <td>
						              <heading>Project</heading>
						            </td>
						          </tr>
						        </table>


						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						        	<!--Diff Net-->
						            <tr>
						            <td width="25%">
						            <div class="two" id='AVE_image'><img src='images/diffnet.png' width="220" height="220"></div>
						            </td>
						            <td valign="top" width="75%">
						            <a href="document/Differential_Network_for_Video_Object_Detection.pdf">
						            <papertitle> Differential Network for Video Object Detection </papertitle>
						            </a>
						            <br>
									<papertitle>Jing Shi</papertitle>,
						            <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
						            <br>
						            <em>CSC 577 <a href="http://www.cs.rochester.edu/~cxu22/t/577F17/">Advanced Topics in Computer Vision</a></em>, 2017
						            <br>
									<a href="https://github.com/jshi31/diffnet">code</a>
						            <p></p>
						            <p align="left">For flow based video obejct detection, we propose the differential network to select the key frame. Then we adapt the sequential NMS to an incremental form so that it is both time-efficient and accurate.</p>
						            </td>
						            </tr>
						        </table>


						        <!-- Intership -->
						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						          <tr>
						            <td>
						              <heading>Internship</heading>
						            </td>
						          </tr>
						        </table>

						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						        	<!--Tencent-->
						            <tr>
						            <td valign="middle" width="25%">
						            <div class="two">
						            <img src='images/tencent.png' width="240" height="110">
						            </div>
						            </td>
						            <td valign="top" width="75%">
						            <papertitle>
						            <a href=https://ai.tencent.com/ailab/index.html>
						            Tencent AI Lab
						            </a> </papertitle>
						            , &nbsp&nbsp
						            Shenzhen, &nbsp&nbsp
						            <em> May - Aug 2018 </em>
						            <br>
						            <p>
						             Collaborator: 
						            <a href="http://pages.cs.wisc.edu/~jiaxu/">Jia Xu</a>,
						            <a href="http://boqinggong.info/">Boqing Gong</a>
						            <br>
						            Project: Weakly supervised video grounding.
						        	</p>
						            </td>
						            </tr>

						        </table>

						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						        	<!--Adobe-->
						            <tr>
						            <td valign="middle" width="25%">
						            <div class="two">
						            <img src='images/adobe_logo.jpg' width="240">
						            </div>
						            </td>
						            <td valign="top" width="75%">
						            <papertitle>
						            <a href=https://research.adobe.com/>
						            Adobe Research
						            </a> </papertitle>
						            , &nbsp&nbsp
						            San Jose, &nbsp&nbsp
						            <em> May - Sep 2019 </em>
						            <br>
						            <p>
						             Collaborator: 
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
						            <br>
						            Project: Language Driven Image Editing.
						        	</p>
						            </td>
						            </tr>
						        </table>

						        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
						        	<!--Adobe-->
						            <tr>
						            <td valign="middle" width="25%">
						            <div class="two">
						            <img src='images/adobe_logo.jpg' width="240">
						            </div>
						            </td>
						            <td valign="top" width="75%">
						            <papertitle>
						            <a href=https://research.adobe.com/>
						            Adobe Research
						            </a> </papertitle>
						            , &nbsp&nbsp
						            San Jose, &nbsp&nbsp
						            <em> May - Sep 2021 </em>
						            <br>
						            <p>
						             Collaborator: 
						            <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
						            <br>
						            Project: Unified Model for Various Image Editing Tasks.
						        	</p>
						            </td>
						            </tr>
						        </table>
        						</td>
    							</tr>
  								</table>


        						</td>
    							</tr>
  								</table>


							  <footer class="major"> </footer>
							</section>



						<!-- Course Section -->
							<section id="course" class="main special">
								<header class="major">
									<h2>Course</h2>
								</header>
									<table width="100" border="1">
									  <tr>
									    <th scope="row">CSC 400 <a href="https://www.cs.rochester.edu/~nelson/courses/csc_400/csc_400.html">Problem Seminar</th>
									    <th scope="row">CSC 440 <a href="http://www.cs.rochester.edu/courses/240/Fall2017.html">Data Mining</a></th>
								      </tr>
									  <tr>
									    <th scope="row">CSC 454 <a href="https://www.cs.rochester.edu/u/scott/courses/254/">Program Language Design and Implementation</a></th>
									    <th scope="row">CSC 577 <a href="http://www.cs.rochester.edu/~cxu22/t/577F17/">Advanced Topics in Computer Vision</a></th>
								      </tr>
									  <tr>
									    <th scope="row">CSC 446 <a href="https://www.cs.rochester.edu/~gildea/2018_Spring/">Machine Learning</a></th>
									    <th scope="row">CSC 484 <a href="http://www.cs.rochester.edu/u/stefanko/Teaching/18CS484/">Advanced Algorithms</a></th>
								      </tr>
									  <tr>
									    <th scope="row">CSC 455 <a href="https://roclocality.org/2017/01/03/cs-255455-spring-2017/">Software Analysis and Improvement</a></th>
									    <th scope="row">CSC 453 <a href="https://roclocality.org/2017/08/18/csc-253-fall-2017/">Dynamic Language and Software Development</a></th>
								      </tr>
									  <tr>
									    <th scope="row">CSC 480 Computing Model and Limitation</th>
									    <th scope="row">CSC 249/449 <a href="https://www.cs.rochester.edu/~cxu22/t/249S19/">Machine Vision</a> [<a href="https://github.com/jshi31/csc249tracking">Teaching Assistant</a>]</th>
								      </tr>

								    </table>
									<p>&nbsp;</p>
								
							  <footer class="major"> </footer>
							</section>


<HR>
<CENTER>
<A HREF="/users"><IMG SRC="/images/up.gif" ALT="UP" BORDER=0></A>
<A HREF="/"><IMG SRC="/images/home.gif" ALT="HOME" BORDER=0></A>
<BR>
<A HREF="/users">URCS People</A> |
<A HREF="/">URCS Home Page</A>
</CENTER>
						

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
